# Creating a Provider
-----
## Prerequisites
 - [NodeJS](https://nodejs.org/en/): CloudCtl is written in typescript and is run using NodeJS.
 - [CDK TF](https://github.com/hashicorp/terraform-cdk): To manage resources we use Terraform. To keep everything clean, we use their CDK framework to keep our code typesafe.
 - Optional [VS Code](https://code.visualstudio.com/); The project comes with recommended style settings and is automatically configured to work with this IDE.
## Setting up the project
To get started we need to clone the repository and install the required dependencies.
```
git clone https://github.com/architect-team/arcctl.git
cd arcctl
npm install
```
From there we can run the project locally using
```
./bin/dev.js --help
```
If everything is working as expected you should see something like
```
arcctl standardizes the interfaces for common cloud resources like VPCs,     managed kubernetes clusters, and more, making it easier for developers to create and manage on-demand cloud infrastructure

VERSION
  @architect-io/arcctl/0.1.0 wsl-x64 node-v17.3.0

USAGE
  $ arcctl [COMMAND]

TOPICS
  create   Create a new cloud resource
  delete   Delete a cloud resource
  get      Get the details of a specific cloud resource
  list     List all the cloud resources matching the specified criteria

COMMANDS
  create         Create a new cloud resource
  delete         Delete a cloud resource
  get            Get the details of a specific cloud resource
  get providers  Get the details of a provider by name
  help           Display help for arcctl.
  list           List all the cloud resources matching the specified criteria
  plugins        List installed plugins.
```
## Create a Provider
Providers are the core building blocks of the framework. A provider allows you to list, get or generate terraform for a pre-defined type on a remote system. For the most part providers tend to map to cloud providers such as AWS, GCP or Azure. Though they can also be used to handle resources on things like a Kubernetes cluster.
### Resource Types
CloudCtl contains a list of predefined resource types that a provider is able to work with. https://github.com/architect-team/arcctl/tree/main/src/%40resources Each of these resources defines what the input and output from the resource type should be. If we look at VPC we can see that it takes in...
```
export type VpcApplyInputs = {
  /** Name of the VPC */
  name: string;
  /** Description for the VPC */
  description?: string;
  /**  Region the VPC exists in */
  region: string;
};
```
So that means any provider that wants to create a VPC will only have access to this information.
If we look at the output for VPC we see the following.
```
export type VpcApplyOutputs = {
name: string;
description?: string;
region: string;
};
```
This means every provider that creates a VPC will need to return this class with the values filled out.
For providers like Digitial Ocean this information is more than enough to get everything working; however, if we look at AWS, we can see that there is a lot more information required to get everything working. In the case of an AWS VPC, we also need to specify subnets. Since our goal is to normalize resource creation across different providers, we have opted to take a more opinionated approach to this problem rather than ask the user different questions per provider. So for AWS VPC subnets, we will just determine what we think the best-case scenarios are and use those to fill in the blanks.
## Actions
For each resource type we can optionally perform the following 3 actions.

 - Get: Takes an id and returns a single resource
 - List: For a given resource type and set of filters, return a list of that resource
 - Terraform: Return an instance of a `BaseTerraformStack` based on either the user defined inputs for the resource type or a known good id for the resource.
 - Converter: In order to delete an object we need two things. The first is the stack that represents the cloud resources and the second is the state. To generate the stack we need to know what values were used to generate it in the first place. So these classes take in an id for the resource generated by the list function and then output what the inputs would have been if the user were to enter them manually. These inputs are then passed to the terraform classes to generate the stack. The state is handled later.
The provider will be able to tell us which of these actions may be performed for each resource type.

## Provider Class
Each provider should have its own folder under `src/@Provider` and no provider should use any code from another provider. These should act as standalone units. This will eventually allow us to make each provider a plugin to let anyone easily create their own.
From there we want to make a new file called `index.ts`. Let's use the AWS provider as an example.
```
export default class AwsProvider extends Provider<AwsCredentials> {
  readonly type: string = 'aws';
  readonly terraform: { [key: string]: ProviderTFGenerator; } = {
    'kubernetes_cluster': new AwsClusterGenerator(this.credentials, this),
    'vpc': new AwsVpcGenerator(this.credentials, this),
  };
  readonly idToInputConverters = {
    'kubernetesCluster': new AwsClusterIdToInputConverter(this.credentials),
    'vpc': new AwsVpcIdToInputConverter(this.credentials),
  };
  readonly list: { [key: string]: ProviderLister<ResourceType> } = {
    'region': new AwsRegionLister(this.credentials),
    'vpc': new AwsVpcLister(this.credentials),
    'kubernetes_version': new AwsKubernetesVersionsLister(this.credentials),
    "node_size": new AwsNodeSizessLister(this.credentials),
    'kubernetes_cluster': new AwsClusterLister(this.credentials),
  }
  readonly get: { [key: string]: ProviderGetter<ResourceType> } = {
    'region': new AwsRegionGetter(this.credentials),
    'vpc': new AwsVpcGetter(this.credentials),
  }
  static CredentialSchema: ProviderCredentialsSchema = {
    access_key_id: {
      message: 'AWS access key ID',
      sensitive: true,
    },
    secret_access_key: {
      message: 'AWS secret access key',
      sensitive: true,
    },
  };
  public testCredentials(): Promise<boolean> {
    return Promise.resolve(true);
  }
}
 ```
The first thing we set is the type. This is a human-readable name that can be used to reference the provider. For instance, if the user wants to create a provider, this is the name that will appear in the list of possible providers.
Next we have `terraform`, `idToInputConverters`, `list` and `get`. Each of these corresponds directly to the actions mentioned above. The key is the resource type such as `vpc`, and the value is a class that extends the corresponding abstract classes linked above.
Next, we have the `CredentialSchema`, which defines the required information needed for us to act on behalf of this provider. For AWS, we need a `key id` and a `secret key`. So when a user asks to create a new provider, we will prompt them to enter this information. We then store the information on the filesystem and will pass it to the provider whenever we go to use it.
Finally, we have the `testCredentials` function, which just validates that the credentials that are stored for the provider are valid. This is important for making sure that a user does not spend all their time configuring a resource only for their credentials to not work.
## Terraform Generator
At the heart of the creation and deletion of objects is the `ProviderTerraformGenerator` class that allows you to wrap the Terraforms CDK. While terraform normally relies on a state file, CloudCtl does not. One of our core goals is to allow users to create and delete complex resources on any cloud provider easily. By adding statefulness, we are just adding another layer of complexity for the end user. So instead, if state is needed, it must be rebuilt each time. This puts extra pressure on the developer working on the provider but will provide a better user experience in the long term.
This class has one important functions that must be created as well as event methods that can be used to perform extra logic during the lifecycle.
### generateTerraform
This function takes in the `ResourceInputs` mentioned earlier and spits out a `BaseTerraformStack`. All the information that you will be able to get from the user is in that object. So remember to be opinionated and help the user down the best possible path. The `BaseTerraformStack` should be a complete Terraform stack. This means it should include backends as well as any resource generation you need.
### Testing CDK
When running the create or delete command you can run them with `--dev`. This will generate the terraform files, but not actually do anything with them. This can let you test them out and tweak them if things do not work as expected. The recommended approach is to run create with `--dev` first. Then go to `~/.arcctl/tf/` and `apply` the terraform yourself. Once that is done you can run `destroy`.
Once you feel comfortable, you can run create with the `--dev` flag and let cloudctl handle the full lifecycle. Before you test delete however, you should backup the `~/.arcctl/tf/` folder somewhere. This will help you manually destroy the resources later if something goes wrong. Once you have that backed up, testing the delete functionality is the same as the create functionality.
One small note. The `main.tf.json` file contains a path that must be updated if you move it somewhere else. You will find something that looks like this.
```
"backend": {
  "local": {
    "path": "/home/muesch/.config/arcctl/tf/tfstate"
  }
},
```
Just make sure to update that path to wherever you move the files to. Otherwise it will not work as expected.
## Converters
As mentioned quite a bit, CloudCtl is statless. That means that if we go tunr the `delete` command for a resource, the only thing we know is which resource the user wants to delete. We do not have the stack and we do not have the statefile. Below we will show how to generate the statefile using the `getImports` functionality, but before we can do that we need to regenerate the terraform stack. Since the terraform stack should be identical between creation and deletion, all we need to do to generate the stack is get the user inputs that would have been used to make the stack. So using the `id` we can query the API of the cloud provider for all the information we need.
### generateTFWithId
The `id` provided is generated by the same providers `list` functionality. So if the user wishes to delete a `vpc` then we first show them a list of options. That list is generated by asking the provider for a list of all vpcs. This means you get to choose the id sent into this application. For a provider like AWS that always needs a region a vpc id may look like `us-east-2/vpc-<some value>`. When we enter this function we can split on the `/` to get the region and vpc id that we need. After using those values to query the AWS api we can rebuild the `ResourceInputs['vpc']` object that represents the stack we need to make.
## Using Terraform CDK
The first step to using Terraform CDK begins with figuring out what terraform you are looking to generate. So before even touching the CDK, start by going into a new folder and building out your desired Terraform experience.
Lets use the basic example below
```hcl
provider "aws" {
  region = local.region
}

locals {
  name   = "ex-${replace(basename(path.cwd), "_", "-")}"
  region = "eu-west-1"
}

module "vpc" {
  source = "git@github.com/terraform-aws-modules/terraform-aws-vpc"
  name = local.name
  cidr = "10.0.0.0/16"
  azs             = ["${local.region}a", "${local.region}b", "${local.region}c"]
  private_subnets = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
  public_subnets  = ["10.0.101.0/24", "10.0.102.0/24", "10.0.103.0/24"]
  enable_ipv6 = true
  enable_nat_gateway = false
  single_nat_gateway = true
  public_subnet_tags = {
    Name = "overridden-name-public"
  }
  vpc_tags = {
    Name = "vpc-name"
  }
}
```
As you can see we have the provider and we are using the vpc module to create the actual resources.
Lets say that as `main.tf`. All terraform must be in a single file for the next steps, so do not break it up into multiple files. Once you have done that make sure to test out that the terraform works as expected. Make sure you can `apply` and `destroy` everything. Take your time to make sure you get this right as if you need to make changes here later, it can require redoing a lot of the following steps.
Also make not of the resource ids that are generated as they will come into play later as well. They will look like
```
module.my_vpc.aws_vpc.this[0]
```

Once you have finished we can conver `HCL` file directly to typescript using `cdktf convert`. A crucial part here is to make sure that you specify the `provider that you are using. So for example to convert the above we would run
```
cat main.tf | cdktf convert --provider hashicorp/aws > imported.ts
```
Which will convert `main.tf` to typescript and save it in `imported.ts`. Other files will be generated as part of the process but those are not important. The `imported.ts` file will look like the following
```
import * as aws from "./.gen/providers/aws";
import * as Vpc from "./.gen/modules/terraform-aws-modules/aws/vpc";
const name = 'ex-${replace(basename(path.cwd), "_", "-")}';
const region = "eu-west-1";
new Vpc.Vpc(this, "vpc", {
  azs: [`\${${region}}a`, `\${${region}}b`, `\${${region}}c`],
  cidr: "10.0.0.0/16",
  enableIpv6: true,
  enableNatGateway: false,
  name: name,
  privateSubnets: ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"],
  publicSubnetTags: [
    {
      Name: "vpc-name",
    },
  ],
  publicSubnets: ["10.0.101.0/24", "10.0.102.0/24", "10.0.103.0/24"],
  singleNatGateway: true,
  vpcTags: [
    {
      Name: "vpc-name",
    },
  ],
});
new aws.provider.AwsProvider(this, "aws", {
  region: region,
});
```
If you look at the top you will see 2 imports that are accessing local files. These are generated by the cdktf command line and provide the type saftey for typescript. The only thing that really matters to us though is the `new Vpc.Vpc` and `new aws.provider` functions. Those are doing the heavy lifting here.
We now have the building blocks to create a `BaseTerraformStack`.
Let's go to the root of the CloudCtl project and look for `cdktf.json`. If you open it up you will see a list of all the required providers and modules we currently use in the application. Here is where you want to add the `providers` and `modules` you used above.
Now we need to get all source files for the new providers so we are going to do the following
```
cdktf get --language typescript
rm -rf ./src/gen
cp -r ./.gen ./src/gen
```
Now we can generate our `BaseTerraformStack` class.
You can view the final version of what we are building [here](https://github.com/architect-team/arcctl/blob/tf_test/src/%40providers/aws/stacks/vpc.ts).
### Constructor
```
constructor(
    scope: Construct,
    private readonly credentials: AwsCredentials,
    private readonly inputs: VpcApplyInputs,
  ) {
    super(scope, inputs.name, inputs.region, credentials);

    let ip_range = inputs.ip_range || '10.0.0.0/16';
    if (ip_range.indexOf('/') === -1) {
      ip_range += '/16';
    }

    const [ip] = ip_range.split('/');
    const ip_parts = ip.split('.').map((ele) => parseInt(ele));

    const privateSubnets: string[] = [];
    const publicSubnets: string[] = [];

    for (let i = 0; i < 3; i++) {
      privateSubnets.push(`${ip_parts.join('.')}/24`);
      ip_parts[2]++;
    }
    for (let i = 0; i < 3; i++) {
      publicSubnets.push(`${ip_parts.join('.')}/24`);
      ip_parts[2]++;
    }

    const allAvailabilityZones =
      new DataAwsAvailabilityZones(
        this,
        "all-availability-zones",
        {}
      ).names;

    this.vpc = new Vpc(this, "vpc",
      {
        azs: allAvailabilityZones,
        name: inputs.name,
        cidr: ip_range,
        privateSubnets,
        publicSubnets,
        enableDnsHostnames: true,
        enableNatGateway: true,
        singleNatGateway: true,
        publicSubnetTags: {
          "kubernetes.io/role/elb": "1",
        },
        privateSubnetTags: {
          "kubernetes.io/role/internal-elb": "1",
        },
      }
    );

    this.vpcid_output = new TerraformOutput(this, "vpcid", {
      value: this.vpc.vpcIdOutput,
    });
  }
```
Our constructor is responsible for building the stack and getting it ready to be used. Once the object has been created it may be used at any time in order to generate the HCL we need to create the resources. So as you can see from the above example we need to make sure our stack is complete by the end of the function. This means that all providers, modules, resources and outputs need to be declared.
You can see we have taken the output of the `cdktf convert` from earlier and just cleaned it up.
### Get Logical ID
When CDK creates a resource it contains 2 parts. The first part is the provided name and the second part is a random 8 characte string. Because of this most resource ids that CDK generates will look like
```
module.my_vpc_845JBNC3.aws_vpc.this[0]
```
In order to reference that resource id we need to use the `getLogicalId` function.
So that instead becomes
```
`module.${this.getLogicalId(this.vpc)}.aws_vpc.this[0]`
```
`this.vpc` relates to the `new Vpc` we created in the constructor.
### Display Outputs
During the creation and deletion process we do not want to display the Terraform logs directly. There is a lot going on and a lot of it will just be noise. So instead we want to provide the user with a more controlled experience. So we will ask the stack to provide a mapping of terraform resource ids to currated display names. The VPC class does the following
```
return {
      [`module.${this.getLogicalId(this.vpc)}.aws_vpc.this[0]`]: "VPC",
      [`module.${this.getLogicalId(this.vpc)}.aws_nat_gateway.this[0]`]: "NAT Gateway",
};
```
When we see the resource id in the terraform logs we will let the user know that we are creating or destroying that resource using the provided display name.
### Get Outputs
After we run terraform apply there may be values that are created that you need access too in order to generate your `ResourceOuputs` object. To handle this, after the `apply` method for Terraform has finished we will call the `getOutputs` method on your stack. Which for the VPC works as follows.
```
  public async getOutputs(): Promise<void> {
    this.id = await Terraform.getOutput(this.vpcid_output.friendlyUniqueId) || '';
  }
```
Earlier in the constructor we created a `TerraformOutput` call `vpcid_output` and added it to the stack. Now we are passing its `friendlyUniqueId` which is akin to its resource id to the Terraform class. From there we will get back a string that we place into a public variable on our stack that can be used later by our generator.
### Get Imports
Earlier we talked about how we do not save the state that terraform produces. Though in order to delete resources we need a mechanism that lets us rebuild that state. This is the function that handles that. Lets break down an example.
```
public async getImports(): Promise<{ [resource_id: string]: string; }> {
    const routeIds = await AwsUtils.getRouteIdsForVpc(this.credentials, this.inputs.region, this.id);
    const subnetIds = await AwsUtils.getSubnetIdsForVpc(this.credentials, this.inputs.region, this.id);
    const natGateways = await AwsUtils.getNatGateways(this.credentials, this.inputs.region, this.id);
    const eipId = await AwsUtils.getElasticIPs(this.credentials, this.inputs.region, this.inputs.name);
    const values = {
      [`module.${this.getLogicalId(this.vpc)}.aws_vpc.this[0]`]: this.id,
      [`module.${this.getLogicalId(this.vpc)}.aws_route_table.public[0]`]: routeIds.public[0],
      [`module.${this.getLogicalId(this.vpc)}.aws_route_table.private[0]`]: routeIds.private[0],
      [`module.${this.getLogicalId(this.vpc)}.aws_subnet.public[0]`]: subnetIds.public[0],
      [`module.${this.getLogicalId(this.vpc)}.aws_subnet.public[1]`]: subnetIds.public[1],
      [`module.${this.getLogicalId(this.vpc)}.aws_subnet.public[2]`]: subnetIds.public[2],
      [`module.${this.getLogicalId(this.vpc)}.aws_subnet.private[0]`]: subnetIds.private[0],
      [`module.${this.getLogicalId(this.vpc)}.aws_subnet.private[1]`]: subnetIds.private[1],
      [`module.${this.getLogicalId(this.vpc)}.aws_subnet.private[2]`]: subnetIds.private[2],
      [`module.${this.getLogicalId(this.vpc)}.aws_internet_gateway.this[0]`]: routeIds.publicGatewayIds[0],
      [`module.${this.getLogicalId(this.vpc)}.aws_nat_gateway.this[0]`]: natGateways[0],
      [`module.${this.getLogicalId(this.vpc)}.aws_eip.nat[0]`]: eipId,
    };
    return values;
  }
```
We can find the resources we need to import by looking at the `apply` function from earlier. Generally speaking if it has the word `data` in it we do not need to import it. Otherwise there is decent chance we will need to.
So for each of the resource ids you saw from the `apply` function, go to your cloud provider SDK and figure out how to get the corresponding id used on their platform. Once we have all of those we just create a list that maps the terraform resource id to the cloud provider resource id.
To get a sense of what the cloud provider resource id is, we can take a look at the documentation. So for `aws_vpc` we can google `aws_vpc terraform` to find it sdocumentation. Once there scroll down to the import section. https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc#import
This will provide you with the id that is expected from the cloud provider.
